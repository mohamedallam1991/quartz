<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="#article/toread
Source : https://www.forbes.com/sites/robtoews/2022/02/13/language-is-the-next-great-frontier-in-ai/?sh=66bb30cf5c50 author: [[Rob Toews]]
Language is the cornerstone of human intelligence.
The emergence of language was the most important intellectual development in our species‚Äô history."><title>ü™¥ Quartz 3.2</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://mohamedallam1991.github.io/quartz//icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><link href=https://mohamedallam1991.github.io/quartz/styles.8010c0d8fb32a34f00886b05df3d230c.min.css rel=stylesheet><script src=https://mohamedallam1991.github.io/quartz/js/darkmode.46b07878b7f5d9e26ad7a3c40f8a0605.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://mohamedallam1991.github.io/quartz/js/popover.688c5dcb89a57776d7f1cbeaf6f7c44b.min.js></script>
<script>const BASE_URL="https://mohamedallam1991.github.io/quartz/",fetchData=Promise.all([fetch("https://mohamedallam1991.github.io/quartz/indices/linkIndex.e96412d4c4048d7c278ebcad5ecdfd4c.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://mohamedallam1991.github.io/quartz/indices/contentIndex.1c32b3172c9ae197ff18035306e9decb.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const n=new URL("https://mohamedallam1991.github.io/quartz/"),s=n.pathname,o=window.location.pathname,i=s==o,e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=i&&!1;drawGraph("https://mohamedallam1991.github.io/quartz",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2}),initPopover("https://mohamedallam1991.github.io/quartz",!0,!0)},init=(e=document)=>{renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/mohamedallam1991.github.io\/quartz\/js\/router.557a499829be51f9008c6efa5b73602a.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-XYFD95KB4J"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XYFD95KB4J",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://mohamedallam1991.github.io/quartz/js/search.cf33b507388f3dfd5513a2afcda7af41.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://mohamedallam1991.github.io/quartz/>ü™¥ Quartz 3.2</a></h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><p class=meta>Last updated June 16, 2022</p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><ol><li><a href=#transformers-a-once-in-a-generation-breakthrough>Transformers: A Once-In-A-Generation Breakthrough</a></li><li><a href=#foundation-models>Foundation Models</a></li><li><a href=#beyond-natural-language>Beyond Natural Language</a></li><li><a href=#conclusion>Conclusion</a></li></ol></li></ol></nav></details></aside><p>#article/toread</p><p>Source :
<a href="https://www.forbes.com/sites/robtoews/2022/02/13/language-is-the-next-great-frontier-in-ai/?sh=66bb30cf5c50" rel=noopener>https://www.forbes.com/sites/robtoews/2022/02/13/language-is-the-next-great-frontier-in-ai/?sh=66bb30cf5c50</a>
author: <a href=/quartz/dzwriters/Notes/Noureddine-Notes/Rob-Toews rel=noopener class=internal-link data-src=/quartz/dzwriters/Notes/Noureddine-Notes/Rob-Toews>Rob Toews</a></p><p>Language is the cornerstone of human intelligence.</p><p>The emergence of language was the most important intellectual development in our species‚Äô history. It is through language that we formulate thoughts and communicate them to one another. Language enables us to reason abstractly, to develop complex ideas about what the world is and could be, and to build on these ideas across generations and geographies. Almost nothing about modern civilization would be possible without language.</p><p>Building machines that can understand language has thus been a central goal of the field of artificial intelligence dating back to its earliest days.</p><p>It has proven maddeningly elusive.</p><p>This is because mastering language is what is known as an
<a href=https://en.wikipedia.org/wiki/AI-complete rel=noopener>‚ÄúAI-complete‚Äù problem</a>: that is, an AI that can truly understand language the way a human can would by implication be capable of any other human-level intellectual activity. Put simply, to solve language is to solve AI.</p><p>This profound and subtle insight is at the heart of the
<a href=https://en.wikipedia.org/wiki/Turing_test rel=noopener>‚ÄúTuring test,‚Äù</a> introduced by AI pioneer Alan Turing in
<a href=https://phil415.pbworks.com/f/TuringComputing.pdf rel=noopener>a groundbreaking 1950 paper</a>. Though often critiqued or misunderstood, the Turing test captures a
<a href=https://www.kurzweilai.net/a-wager-on-the-turing-test-why-i-think-i-will-win rel=noopener>fundamental reality</a> about language and intelligence; as it approaches its 75th birthday, it remains as relevant as it was when Turing first conceived it.</p><p>MORE FROMFORBES ADVISOR</p><p>Humanity has yet to build a machine intelligence with human-level mastery of language. (In other words, no machine intelligence has yet passed the Turing test.) But over the past few years researchers have achieved startling, game-changing breakthroughs in language AI, also called natural language processing (NLP).</p><p>The technology is now at a critical inflection point, poised to make the leap from academic research to widespread real-world adoption. In the process, broad swaths of the business world and our daily lives will be transformed. Given language‚Äôs ubiquity, few areas of technology will have a more far-reaching impact on society in the years ahead.</p><a href=#transformers-a-once-in-a-generation-breakthrough><h3 id=transformers-a-once-in-a-generation-breakthrough><span class=hanchor arialabel=Anchor># </span>Transformers: A Once-In-A-Generation Breakthrough</h3></a><p>The most powerful way to illustrate the capabilities of today‚Äôs cutting-edge language AI is to start with a few concrete examples.</p><p>Today‚Äôs AI
<a href="https://twitter.com/QasimMunye/status/1278750809094750211?s=20&t=GN6OnHrn97_Myzyf9nCuRw" rel=noopener>can correctly answer</a> complex medical queries‚Äîand explain the underlying biological mechanisms at play. It
<a href="https://twitter.com/zebulgar/status/1283927560435326976?s=20&t=Z27yBOy0MYNHEV6zjExs1Q" rel=noopener>can craft</a> nuanced memos about how to run effective board meetings. It
<a href=https://maraoz.com/2020/07/18/openai-gpt3/ rel=noopener>can write</a> articles analyzing its own capabilities and limitations, while convincingly pretending to be a human observer. It
<a href=https://www.gwern.net/GPT-3 rel=noopener>can produce</a> original, sometimes beautiful, poetry and literature.</p><p>(It is worth taking a few moments to inspect these examples yourself.)</p><p>What is behind these astonishing new AI abilities, which just five years ago would have been inconceivable?</p><p>In short: the invention of the transformer, a new neural network architecture that has unleashed vast new possibilities in AI.</p><p>A group of Google researchers introduced the transformer in late 2017 in a
<a href=https://arxiv.org/pdf/1706.03762.pdf rel=noopener>now-classic research paper</a>.</p><p>Before transformers, the state of the art in NLP‚Äîfor instance, LSTMs and the widely-used
<a href=https://arxiv.org/pdf/1409.3215.pdf rel=noopener>Seq2Seq architecture</a>‚Äîwas based on recurrent neural networks. By definition, recurrent neural networks process data sequentially‚Äîthat is, one word at a time, in the order that the words appear.</p><p>Transformers‚Äô great innovation is to make language processing <em>parallelized</em>, meaning that all the tokens in a given body of text are analyzed at the same time rather than in sequence. In order to support this parallelization, transformers rely on an AI mechanism known as attention. Attention enables a model to consider the relationships between words, even if they are far apart in a text, and to determine which words and phrases in a passage are most important to ‚Äúpay attention to.‚Äù</p><p>Parallelization also makes transformers vastly more computationally efficient than RNNs, meaning that they can be trained on larger datasets and built with more parameters. One defining characteristic of today‚Äôs transformer models is their massive size.</p><p>A flurry of innovation followed in the wake of the original transformer paper as the world‚Äôs leading AI researchers built upon this foundational breakthrough.</p><p>The publication of the landmark transformer model
<a href=https://arxiv.org/pdf/1810.04805.pdf rel=noopener>BERT</a> came in 2018. Created at Google, BERT‚Äôs big conceptual advance is its bidirectional structure (the B in BERT stands for ‚Äúbidirectional‚Äù). The model ‚Äúlooks in both directions‚Äù as it analyzes a given word, considering both the words that come before and the words that come after, rather than working unidirectionally from left to right. This additional context allows for richer, more nuanced language modeling.</p><p>BERT remains one of the most important transformer-based models in use, frequently treated as a reference against which newer models are compared. Much subsequent research on transformers‚Äîfor instance, Facebook‚Äôs influential
<a href=https://arxiv.org/pdf/1907.11692.pdf rel=noopener>RoBERTa model</a> (2019)‚Äîis based on refining BERT.</p><p>Google‚Äôs entire search engine today is
<a href=https://blog.google/products/search/search-language-understanding-bert/ rel=noopener>powered by BERT</a>, one of the most far-reaching examples of transformers‚Äô real-world impact.</p><p>Another core vein of research in the world of transformers is OpenAI‚Äôs family of GPT models. OpenAI published the
<a href=https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf rel=noopener>original GPT</a> in June 2018,
<a href=https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf rel=noopener>GPT-2</a> in February 2019, and
<a href=https://arxiv.org/pdf/2005.14165.pdf rel=noopener>GPT-3</a> in May 2020. Popular
<a href=https://www.eleuther.ai/ rel=noopener>open-source versions</a> of these models, like GPT-J and GPT-Neo, have followed.</p><p>As the ‚ÄúG‚Äù in their names indicates, the GPT models are generative: they generate original text output in response to the text input they are fed. This is an important distinction between the GPT class of models and the BERT class of models. BERT, unlike GPT, does not generate new text but instead analyzes existing text (think of activities like search, classification, or sentiment analysis).</p><p>GPT‚Äôs generative capabilities make these models particularly attention-grabbing, since writing appears to be a creative act and the output can be astonishingly human-like. Text generation is sometimes referred to as ‚ÄúNLP‚Äôs party trick.‚Äù (All four of the examples linked to above are text generation examples from GPT-3.)</p><p>Perhaps the most noteworthy element of the GPT architecture is its sheer size. OpenAI has been intentional and transparent about its strategy to pursue more advanced language AI capabilities through raw scale above all else: more compute, larger training data corpora, larger models.</p><p>With 1.5 billion parameters, GPT-2 was the largest model ever built at the time of its release. Published less than a year later, GPT-3 was two orders of magnitude larger: a whopping 175 billion parameters.
<a href=https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/ rel=noopener>Rumors have circulated</a> that GPT-4 will contain on the order of <em>100 trillion parameters</em> (perhaps not coincidentally, roughly equivalent to
<a href=https://www.scientificamerican.com/article/100-trillion-connections/ rel=noopener>the number of synapses</a> in the human brain). As a point of comparison, the largest BERT model had 340 million parameters.</p><p>As with any machine learning effort today, the performance of these models depends above all on the data on which they are trained.</p><p>Today‚Äôs transformer-based models learn language by ingesting essentially the entire internet. BERT was fed all of Wikipedia (along with the digitized texts of thousands of unpublished books). RoBERTa improved upon BERT by training on even larger volumes of text from the internet. GPT-3‚Äôs training dataset was larger still, consisting of half a trillion language tokens. Thus, these models‚Äô linguistic outputs and behaviors can ultimately be traced to the statistical patterns in all the text that humans have previously published online.</p><p>The reason such large training datasets are possible is that transformers use self-supervised learning, meaning that they learn from unlabeled data. This is a crucial difference between today‚Äôs cutting-edge language AI models and the previous generation of NLP models, which had to be trained with labeled data. Today‚Äôs self-supervised models can train on far larger datasets than was ever previously possible: after all, there is more unlabeled text data than labeled text data in the world by many orders of magnitude.</p><p>Some observers point to self-supervised learning, and the vastly larger training datasets that this technique unlocks, as the single most important driver of NLP‚Äôs dramatic performance gains in recent years, more so than any other feature of the transformer architecture.</p><a href=#foundation-models><h3 id=foundation-models><span class=hanchor arialabel=Anchor># </span>Foundation Models</h3></a><p>Training models on massive datasets with millions or billions of parameters requires vast computational resources and engineering know-how. This makes large language models prohibitively costly and difficult to build. GPT-3, for example, required several thousand petaflop/second-days to train‚Äîa staggering amount of computational resources.</p><p>Because very few organizations in the world have the resources and talent to build large language models from scratch, almost all cutting-edge NLP models today are adapted from a small handful of base models: e.g., BERT, RoBERTa, GPT-2, BART. Almost without exception, these models come from the world‚Äôs largest tech companies: Google, Facebook, OpenAI (which is bankrolled by Microsoft), Nvidia.</p><p>Without anyone quite planning for it, this has resulted in an entirely new paradigm for NLP technology development‚Äîone that will have profound implications for the nascent AI economy.</p><p>This paradigm can be thought of in two basic phases: pre-training and fine-tuning.</p><p>In the first phase, a tech giant creates and open-sources a large language model: for instance, Google‚Äôs BERT or Facebook‚Äôs RoBERTa.</p><p>Unlike in previous generations of NLP, in which models had to be built for individual language tasks, these massive models are not specialized for any particular activity. They have powerful generalized language capabilities across functions and topic areas. Out of the box, they perform well at the full gamut of activities that comprise linguistic competence: language classification, language translation, search, question answering, summarization, text generation, conversation. Each of these activities on its own presents compelling technological and economic opportunities.</p><p>Because they can be adapted to any number of specific end uses, these base models are referred to as ‚Äúpre-trained.‚Äù</p><p>In the second phase, downstream users‚Äîyoung startups, academic researchers, anyone else who wants to build an NLP model‚Äîtake these pre-trained models and refine them with a small amount of additional training data in order to optimize them for their own specific use case or market. This step is referred to as ‚Äúfine-tuning.‚Äù</p><p>‚ÄúToday‚Äôs pre-trained models are incredibly powerful, and even more importantly, they are publicly available,‚Äù said Yinhan Liu, lead author on Facebook‚Äôs RoBERTa work and now cofounder/CTO of healthcare NLP startup BirchAI. ‚ÄúFor those teams that have the know-how to operationalize transformers, the question becomes: what is the most important or impactful use case to which I can apply this technology?‚Äù</p><p>Under this ‚Äúpre-train then fine-tune‚Äù paradigm, the heavy lifting is done upfront with the creation of the pre-trained model. Even after fine-tuning, the end model‚Äôs behavior remains largely dictated by the pre-trained model‚Äôs parameters.</p><p>This makes these pre-trained models incredibly influential. So influential, in fact, that Stanford University has recently coined a new name for them, ‚Äúfoundation models‚Äù, and launched an entire academic program devoted to better understanding them: the
<a href=https://crfm.stanford.edu/ rel=noopener>Center for Research on Foundation Models</a> (CRFM). The Stanford team believes that foundation models, and the small group of tech giants that have the resources to produce them, will exert outsize influence on the future behavior of artificial intelligence around the world.</p><p>As the researchers
<a href=https://arxiv.org/pdf/2108.07258.pdf rel=noopener>put it</a>: ‚ÄúFoundation models have led to an unprecedented level of homogenization: Almost all state-of-the-art NLP models are now adapted from one of a few foundation models. While this homogenization produces extremely high leverage (any improvements in the foundation models can lead to immediate benefits across all of NLP), it is also a liability; all AI systems might inherit the same problematic biases of a few foundation models.‚Äù</p><p>This Stanford effort is drawing attention to a massive looming problem for large language models: social bias.</p><p>The source of social bias in AI models is straightforward to summarize but insidiously difficult to root out. Because large language models (or foundation models, to use the new branding) learn language by ingesting what humans have written online, they inevitably inherit the prejudices, false assumptions and harmful beliefs of their imperfect human progenitors. Just imagine all the fringe subreddits and bigoted blogs that must have been included in GPT-3‚Äôs vast training data corpus.</p><p>The problem has been
<a href=https://arxiv.org/pdf/2106.13219.pdf rel=noopener>extensively documented</a>: today‚Äôs most prominent foundation models all exhibit racist, sexist, xenophobic, and other antisocial tendencies. This issue will only grow more acute as foundation models become increasingly influential in society. Some observers believe that AI bias will eventually become as prominent of an issue for consumers, companies and governments as digital threats like data privacy or cybersecurity that have come before it‚Äîthreats that were also not fully appreciated at first, because the breakneck pace of technological change outstripped society‚Äôs ability to properly adapt to it.</p><p>There is no silver-bullet solution to the challenge of AI bias and toxicity. But as the problem becomes more widely recognized, a number of mitigation strategies are being pursued.</p><p>Last month, OpenAI
<a href=https://openai.com/blog/instruction-following/ rel=noopener>announced</a> that it had developed a new version of GPT-3 that is ‚Äúsafer, more helpful, and more aligned‚Äù with human values. The company used a technique known as
<a href=https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf rel=noopener>reinforcement learning from human feedback</a> to fine-tune its models to be less biased and more truthful than the original GPT-3. This new version, named InstructGPT, is now the default language model that OpenAI makes available to customers.</p><p>Historically, Alphabet‚Äôs DeepMind has been an outlier among the world‚Äôs most advanced AI research organizations for <em>not</em> making language AI a major focus area. This changed at the end of 2021, with DeepMind
<a href=https://deepmind.com/blog/article/language-modelling-at-scale rel=noopener>announcing</a> a collection of important work on large language models.</p><p>Of the three NLP papers that DeepMind published, one is devoted entirely to the ethical and social risks of language AI. The paper
<a href=https://arxiv.org/pdf/2112.04359.pdf rel=noopener>proposes</a> a comprehensive taxonomy of 6 thematic areas and 21 specific risks that language models pose, including discrimination, exclusion, toxicity and misinformation. DeepMind pledged to make these risks a central focus of its NLP research going forward to help ensure that it is pursuing innovation in language AI responsibly.</p><p>The fact that this dimension of language AI research‚Äîuntil recently, treated as an afterthought or ignored altogether by most of the world‚Äôs NLP researchers‚Äîfeatured so centrally in DeepMind‚Äôs recent foray into language AI may be a signal of the field‚Äôs shifting priorities moving forward.</p><p>Increased regulatory focus on the harms of bias and toxicity in AI models will only accelerate this shift. And make no mistake: regulatory action on this front is a matter of when, not if.</p><a href=#beyond-natural-language><h3 id=beyond-natural-language><span class=hanchor arialabel=Anchor># </span>Beyond Natural Language</h3></a><p>Interestingly, perhaps the most creative use cases for NLP today don‚Äôt involve natural language at all. In particular, today‚Äôs cutting-edge language AI technology is powering remarkable breakthroughs in two other domains: coding and biology.</p><p>Whether it‚Äôs Python, Ruby, or Java, computer programming happens via languages. Just like natural languages like English or Swahili, programming languages are symbolically represented, follow regular rules, and have a robust internal logic. The audience just happens to be software compilers rather than other humans.</p><p>It therefore makes sense that the same powerful new technologies that have given AI incredible fluency in natural language can likewise be applied to programming languages, with similar results.</p><p>Last summer OpenAI
<a href=https://arxiv.org/pdf/2107.03374.pdf rel=noopener>announced Codex</a>, a transformer-based model that can write computer code astonishingly well. In parallel, GitHub (which is allied with OpenAI through its parent company Microsoft) launched a productized version of Codex
<a href=https://github.blog/2021-06-29-introducing-github-copilot-ai-pair-programmer/ rel=noopener>named Copilot</a>.</p><p>To develop Codex, OpenAI took GPT-3 and fine-tuned it on a massive volume of publicly available written code from GitHub.</p><p>Codex‚Äôs design is simple: human users give it a plain-English description of a command or function and Codex turns this description into functioning computer code. A user could input into Codex, for instance, ‚Äúcrop this image circularly‚Äù or ‚Äúanimate this image horizontally so that it bounces off the left and right walls‚Äù‚Äîand Codex can produce a snippet of code to implement those actions. (These exact examples can be examined on
<a href=https://openai.com/blog/openai-codex/#spacegame rel=noopener>OpenAI‚Äôs website</a>.) Codex is most capable in Python, but it is proficient in over a dozen programming languages.</p><p>Then, just two weeks ago, DeepMind further advanced the frontiers of AI coding with its
<a href=https://www.deepmind.com/blog/article/Competitive-programming-with-AlphaCode rel=noopener>publication of AlphaCode</a>.</p><p>AlphaCode is an AI system that can compete at a human level in programming competitions. In these competitions, which attract hundreds of thousands of participants each year, contestants receive a lengthy problem statement in English and must construct a complete computer program that solves it. Example problems include devising strategies for a custom board game or solving an arithmetic-based brain teaser.</p><p>While OpenAI‚Äôs Codex can produce short snippets of code in response to concrete descriptions, DeepMind‚Äôs AlphaCode goes much further. It begins to approach the full complexity of real-world programming: assessing an abstract problem without a clear solution, devising a structured approach to solving it, and then executing on that approach with up to hundreds of lines of code. AlphaCode almost seems to display that ever-elusive attribute in AI, high-level reasoning.</p><p>As DeepMind‚Äôs AlphaCode team
<a href=https://www.deepmind.com/blog/article/Competitive-programming-with-AlphaCode rel=noopener>wrote</a>: ‚ÄúCreating solutions to unforeseen problems is second nature in human intelligence‚Äîa result of critical thinking informed by experience. For artificial intelligence to help humanity, our systems need to be able to develop problem-solving capabilities. AlphaCode solves new problems in programming competitions that require a combination of critical thinking, logic, algorithms, coding, and natural language understanding.‚Äù</p><p>Another ‚Äúlanguage‚Äù in which today‚Äôs cutting-edge NLP has begun to generate remarkable insights is biology, from genomics to proteins.</p><p>Genomics is well-suited to the application of large language models because an individual‚Äôs entire genetic endowment is encoded in a simple four-letter alphabet: A (for adenine), C (for cytosine), G (for guanine), and T (for thymine). Every human‚Äôs DNA is defined by a string of billions of A‚Äôs, C‚Äôs, G‚Äôs and T‚Äôs (known as nucleotides) in a particular order.</p><p>In many respects DNA functions like a language, with its nucleotide sequences exhibiting regular patterns that resemble a kind of vocabulary, grammar, and semantics. What does this language say? It defines much about who we are, from our height to our eye color to our risk of heart disease or substance abuse.</p><p>Large language models are now making rapid progress in deciphering the language of DNA, in particular its ‚Äúnoncoding‚Äù regions. These noncoding regions do not contain genes but rather <em>control</em> genes: they regulate how much, when, and where given genes are expressed, giving them a central role in the maintenance of life. Noncoding regions comprise 98% of our total DNA but until now have remained poorly understood.</p><p>A few months ago, DeepMind
<a href=https://deepmind.com/blog/article/enformer rel=noopener>introduced</a> a new transformer-based architecture that can predict gene expression based on DNA sequence with unprecedented accuracy. It does so by considering interactions between genes and noncoding DNA sequences at much greater distances than was ever before possible. A team at Harvard completed
<a href=https://towardsdatascience.com/bringing-bert-to-the-field-how-to-predict-gene-expression-from-corn-dna-9287af91fcf8 rel=noopener>work</a> along similar lines to better understand gene expression in corn (fittingly naming their model ‚ÄúCornBERT‚Äù).</p><p>Another subfield of biology that represents fertile ground for language AI is
<a href=https://www.sciencedirect.com/science/article/pii/S2001037021000945#! rel=noopener>the study of proteins</a>. Proteins are strings of building blocks known as amino acids, linked together in a particular order. There are 20 amino acids in total. Thus, for all their complexity, proteins can be treated as tokenized strings‚Äîwherein each amino acid, like each word in a natural language, is a token‚Äîand analyzed accordingly.</p><p>As one example, an AI research team from Salesforce
<a href=https://blog.salesforceairesearch.com/progen/ rel=noopener>recently built</a> an NLP model that ‚Äúlearns the language of proteins‚Äù and can generate plausible protein sequences that don‚Äôt exist in nature with prespecified characteristics. The potential applications of this type of controllable protein synthesis are tantalizing.</p><p>These efforts are just the beginning. In the months and years ahead, language AI will make profound contributions to our understanding of how life itself works.</p><a href=#conclusion><h3 id=conclusion><span class=hanchor arialabel=Anchor># </span>Conclusion</h3></a><p>Language is at the heart of human intelligence. It therefore is and must be at the heart of our efforts to build artificial intelligence. No sophisticated AI can exist without mastery of language.</p><p>Today, the field of language AI is at an exhilarating inflection point, on the cusp of transforming industries and spawning new multi-billion-dollar companies. At the same time, it is fraught with societal dangers like bias and toxicity that are only now starting to get the attention they deserve.</p><p>This article explored the big-picture developments and trends shaping the world of language AI today. In a
<a href="https://www.forbes.com/sites/robtoews/2022/03/27/a-wave-of-billion-dollar-language-ai-startups-is-coming/?sh=1e8aa7302b14" rel=noopener>followup article</a>, we canvass today‚Äôs most exciting NLP startups. A growing group of NLP entrepreneurs is applying cutting-edge language AI in creative ways across sectors and use cases, generating massive economic value and profound industry disruption. Few startup categories hold more promise in the years ahead.</p></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://mohamedallam1991.github.io/quartz/js/graph.afdb02e537635f9a611b53a988e5645b.js></script></div></div><div id=contact_buttons><footer><p>Made by Jacky Zhao using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, ¬© 2022</p><ul><li><a href=/>Home</a></li><li><a href=https://twitter.com/_jzhao>Twitter</a></li><li><a href=https://github.com/jackyzha0>Github</a></li></ul></footer></div></div></body></html>